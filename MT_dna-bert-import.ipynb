{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5c10a76-09f2-431a-b169-4e56e5664dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Data processing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Utilities\n",
    "import gc\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "# Set style for prettier plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31316e20-81b0-4ff5-96b2-5a4eb126aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "model = AutoModel.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e9973e8-123c-40e5-8542-012ed602fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       genus       species  \\\n",
       "0               Alitibacter   langaaensis   \n",
       "1               Alitibacter   langaaensis   \n",
       "2               Roseovarius     maritimus   \n",
       "3               Roseovarius        roseus   \n",
       "4           Planosporangium      spinosum   \n",
       "...                     ...           ...   \n",
       "27727     Thermoclostridium  stercorarium   \n",
       "27728           Clostridium      isatidis   \n",
       "27729         Couchioplanes     caeruleus   \n",
       "27730             Halomonas     koreensis   \n",
       "27731  Pseudoflavonifractor   phocaeensis   \n",
       "\n",
       "                                                sequence   identifier  \\\n",
       "0      ATTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCT...  NR_118751.1   \n",
       "1      ATTGAACGCTGGCGGCAGGCTTAACACATGCAAGTCGAACGGTAAC...  NR_042885.1   \n",
       "2      CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...  NR_200035.1   \n",
       "3      CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...  NR_200034.1   \n",
       "4      TTGTTGGAGAGTTTGATCCTGGCTCAGGACGAACGCTGGCGGCGTG...  NR_200033.1   \n",
       "...                                                  ...          ...   \n",
       "27727  TGATCCTGGCTCAGGACGAACGCTGGCGGCGTGCCTAACACATGCA...  NR_025100.1   \n",
       "27728  GGCGTGCNTAACACATGCAAGTCGAGCGAGGTGATTTCNTTCGGGA...  NR_026347.1   \n",
       "27729  CGCTGGCGGCGTGCTTAACACATGCAAGTCGAGCGGAAAGGCCCCT...  NR_026295.1   \n",
       "27730  ACGATGGGAGCTTGCTCCCAGGCGTCGAGCGGCGGACGGGTGAGTA...  NR_025773.1   \n",
       "27731  AGAGTTTGATCCTGGCTCAGGATGAACGCTGGCGGCGTRCTTAACA...  NR_147370.1   \n",
       "\n",
       "             is_complete  \n",
       "0       partial sequence  \n",
       "1       partial sequence  \n",
       "2      complete sequence  \n",
       "3      complete sequence  \n",
       "4      complete sequence  \n",
       "...                  ...  \n",
       "27727   partial sequence  \n",
       "27728   partial sequence  \n",
       "27729   partial sequence  \n",
       "27730   partial sequence  \n",
       "27731   partial sequence  \n",
       "\n",
       "[27732 rows x 5 columns]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the new sequence data and run head to see the overall data structure\n",
    "data_in = pd.read_csv(\"data/sequence-wide.tsv\", sep='\\t') \n",
    "data_in.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb1efdce-56aa-428f-a633-b6d28c915dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 1:\n",
      "0                 Alitibacter\n",
      "1                 Alitibacter\n",
      "2                 Roseovarius\n",
      "3                 Roseovarius\n",
      "4             Planosporangium\n",
      "                 ...         \n",
      "27727       Thermoclostridium\n",
      "27728             Clostridium\n",
      "27729           Couchioplanes\n",
      "27730               Halomonas\n",
      "27731    Pseudoflavonifractor\n",
      "Name: genus, Length: 27732, dtype: object\n",
      "Column 2:\n",
      "0         langaaensis\n",
      "1         langaaensis\n",
      "2           maritimus\n",
      "3              roseus\n",
      "4            spinosum\n",
      "             ...     \n",
      "27727    stercorarium\n",
      "27728        isatidis\n",
      "27729       caeruleus\n",
      "27730       koreensis\n",
      "27731     phocaeensis\n",
      "Name: species, Length: 27732, dtype: object\n",
      "Column 3:\n",
      "0        ATTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCT...\n",
      "1        ATTGAACGCTGGCGGCAGGCTTAACACATGCAAGTCGAACGGTAAC...\n",
      "2        CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...\n",
      "3        CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...\n",
      "4        TTGTTGGAGAGTTTGATCCTGGCTCAGGACGAACGCTGGCGGCGTG...\n",
      "                               ...                        \n",
      "27727    TGATCCTGGCTCAGGACGAACGCTGGCGGCGTGCCTAACACATGCA...\n",
      "27728    GGCGTGCNTAACACATGCAAGTCGAGCGAGGTGATTTCNTTCGGGA...\n",
      "27729    CGCTGGCGGCGTGCTTAACACATGCAAGTCGAGCGGAAAGGCCCCT...\n",
      "27730    ACGATGGGAGCTTGCTCCCAGGCGTCGAGCGGCGGACGGGTGAGTA...\n",
      "27731    AGAGTTTGATCCTGGCTCAGGATGAACGCTGGCGGCGTRCTTAACA...\n",
      "Name: sequence, Length: 27732, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the columns\n",
    "print(\"Column 1:\")\n",
    "print(data_in.iloc[:, 0])\n",
    "print(\"Column 2:\")\n",
    "print(data_in.iloc[:, 1])\n",
    "print(\"Column 3:\")\n",
    "print(data_in.iloc[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90b6a90c-5dc6-4402-b207-c4ec3f1647b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ATTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCT...\n",
       "1        ATTGAACGCTGGCGGCAGGCTTAACACATGCAAGTCGAACGGTAAC...\n",
       "2        CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...\n",
       "3        CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...\n",
       "4        TTGTTGGAGAGTTTGATCCTGGCTCAGGACGAACGCTGGCGGCGTG...\n",
       "                               ...                        \n",
       "27727    TGATCCTGGCTCAGGACGAACGCTGGCGGCGTGCCTAACACATGCA...\n",
       "27728    GGCGTGCNTAACACATGCAAGTCGAGCGAGGTGATTTCNTTCGGGA...\n",
       "27729    CGCTGGCGGCGTGCTTAACACATGCAAGTCGAGCGGAAAGGCCCCT...\n",
       "27730    ACGATGGGAGCTTGCTCCCAGGCGTCGAGCGGCGGACGGGTGAGTA...\n",
       "27731    AGAGTTTGATCCTGGCTCAGGATGAACGCTGGCGGCGTRCTTAACA...\n",
       "Name: sequence, Length: 27732, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71b93154-30bd-411a-a82b-e6dd4026314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2061, 25, 222, 23, 224, 143, 3411, 403, 247, 53, 150, 527, 2759, 2834, 2734, 724, 873, 81, 118, 2470, 30, 708, 72, 61, 679, 29, 200, 88, 2894, 71, 117, 639, 72, 1478, 315, 137, 2787, 825, 1826, 966, 189, 1235, 45, 229, 4079, 314, 1340, 835, 427, 138, 316, 99, 120, 2139, 76, 36, 987, 75, 315, 8, 0, 41, 199, 0, 0, 5, 778, 460, 632, 59, 100, 72, 26, 0, 1212, 1527, 71, 148, 281, 0, 9, 3558, 238, 92, 635, 59, 111, 556, 2787, 135, 52, 259, 64, 72, 31, 120, 469, 2816, 50, 2638, 166, 29, 135, 1262, 31, 141, 17, 495, 1170, 317, 32, 443, 79, 78, 30, 619, 36, 247, 137, 1517, 2810, 19, 153, 1826, 20, 277, 1080, 332, 159, 15, 583, 458, 61, 783, 18, 486, 17, 540, 29, 200, 14, 183, 22, 236, 168, 37, 282, 3453, 71, 7, 0, 3386, 34, 123, 315, 103, 265, 194, 50, 42, 534, 171, 259, 166, 112, 2394, 200, 106, 59, 118, 1952, 409, 577, 117, 124, 1832, 0, 113, 205, 35, 553, 403, 38, 499, 16, 605, 788, 212, 8, 0, 0, 9, 3382, 169, 194, 233, 368, 38, 2785, 1149, 282, 1435, 66, 101, 39, 386, 8, 0, 9, 10, 846, 599, 159, 27, 1317, 208, 3928, 109, 499, 0, 9, 118, 317, 292, 114, 149, 100, 462, 2093, 2641, 14, 163, 2117, 72, 212, 126, 823, 472, 65, 62, 1440, 239, 163, 327, 48, 342, 0, 5, 90, 547, 93, 1818, 1128, 130, 48, 89, 266, 8, 0, 9, 1731, 386, 33, 251, 1957, 8, 0, 9, 438, 57, 885, 245, 1191, 67, 224, 3953, 93, 522, 3315, 123, 37, 238, 30, 104, 2276, 247, 166, 74, 393, 61, 841, 116, 3345, 278, 35, 90, 216, 33, 220, 2485, 3770, 52, 41, 57, 2957, 68, 16, 124, 455, 48, 247, 1092, 134, 716, 184, 2146, 1002, 397, 2596, 824, 101, 35, 637, 3642, 540, 347, 165, 764, 268, 72, 252, 115, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "# Convert the new sequences to input tokens, view the first element\n",
    "inputs = tokenizer(data_in.sequence.to_list())[\"input_ids\"]\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb5d480c-bc7c-4a53-af57-4f096a1deedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Random seeds set for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Set all random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3148d170-34b8-472b-b413-96254b368c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Applying LoRA adapters to model...\n",
      "\n",
      "ðŸ“Š Model Parameters Comparison:\n",
      "------------------------------------------------------------\n",
      "Before LoRA (original model):\n",
      "  Trainable params: 294,912\n",
      "  Total params: 89,481,984\n",
      "  Trainable: 0.33%\n",
      "\n",
      "After LoRA (adapted model):\n",
      "  Trainable params: 294,912\n",
      "  Total params: 89,481,984\n",
      "  Trainable: 0.33%\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ“ LoRA adapters applied successfully!\n"
     ]
    }
   ],
   "source": [
    "# Helper function to count trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Count and display trainable vs total parameters.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    trainable_pct = 100 * trainable_params / all_param\n",
    "    print(f\"  Trainable params: {trainable_params:,}\")\n",
    "    print(f\"  Total params: {all_param:,}\")\n",
    "    print(f\"  Trainable: {trainable_pct:.2f}%\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                              # Rank (lower = fewer params)\n",
    "    lora_alpha=32,                    # Scaling factor\n",
    "    target_modules=[\"query\", \"value\"], # Adapt attention layers\n",
    "    lora_dropout=0.1,                 # Regularization\n",
    "    bias=\"none\",                      # Don't adapt bias terms\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(\"ðŸ”„ Applying LoRA adapters to model...\\n\")\n",
    "ft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"ðŸ“Š Model Parameters Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Before LoRA (original model):\")\n",
    "print_trainable_parameters(model)\n",
    "print(\"\\nAfter LoRA (adapted model):\")\n",
    "print_trainable_parameters(ft_model)\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nâœ“ LoRA adapters applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58a1f380-a9fc-4bdf-9c71-e61d4272d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train-val-test splits\n",
    "# Configuration\n",
    "prop_train = 0.8\n",
    "prop_val = 0.1\n",
    "prop_test = 0.1\n",
    "SEED = 42\n",
    "\n",
    "# Create random splits\n",
    "rng = np.random.default_rng(SEED)\n",
    "random_idxs = rng.permutation(len(data_in))\n",
    "\n",
    "# Calculate split sizes\n",
    "n_total = len(data_in)\n",
    "n_train = int(prop_train * n_total)\n",
    "n_val = int(prop_val * n_total)\n",
    "\n",
    "train_df = data_in.iloc[random_idxs[:n_train]]\n",
    "val_df = data_in.iloc[random_idxs[n_train:n_train + n_val]]\n",
    "test_df = data_in.iloc[random_idxs[n_train + n_val:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f2e83bc-15ed-44cf-9c45-c9bd606a348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SequenceDataset class defined\n"
     ]
    }
   ],
   "source": [
    "# Custom PyTorch Dataset for protein sequences\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that tokenizes protein sequences on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sequence = row[\"sequence\"]\n",
    "        label = torch.tensor(row[\"genus\"], dtype=torch.float32)\n",
    "\n",
    "        # Tokenize sequence\n",
    "        inputs = self.tokenizer(\n",
    "            sequence,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        # Remove batch dimension\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs, label\n",
    "\n",
    "print(\"âœ“ SequenceDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c482cd2-2279-4909-90a1-4923849ab3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Datasets created:\n",
      "  Train: 22,185 samples\n",
      "  Val: 2,773 samples\n",
      "  Test: 2,774 samples\n"
     ]
    }
   ],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = SequenceDataset(train_df, tokenizer)\n",
    "val_dataset = SequenceDataset(val_df, tokenizer)\n",
    "test_dataset = SequenceDataset(test_df, tokenizer)\n",
    "\n",
    "print(\"âœ“ Datasets created:\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples\")\n",
    "print(f\"  Val: {len(val_dataset):,} samples\")\n",
    "print(f\"  Test: {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07e15da4-24e8-4722-87b6-173918eb9247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "âœ“ Label encoding complete:\n",
      "  Number of unique species: 4318\n",
      "  Example: ''Burkholderia' -> 0\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 128  \n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Create Label Encodings for Species\n",
    "# =============================================================================\n",
    "\n",
    "# Encode species names as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data_in['genus'])\n",
    "\n",
    "# Add encoded labels to dataframes\n",
    "train_df = train_df.copy()\n",
    "val_df = val_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "train_df['genus_encoded'] = label_encoder.transform(train_df['genus'])\n",
    "val_df['genus_encoded'] = label_encoder.transform(val_df['genus'])\n",
    "test_df['genus_encoded'] = label_encoder.transform(test_df['genus'])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"\\nâœ“ Label encoding complete:\")\n",
    "print(f\"  Number of unique species: {num_classes}\")\n",
    "print(f\"  Example: '{label_encoder.classes_[0]}' -> {0}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Update Dataset Class for Classification\n",
    "# =============================================================================\n",
    "class SequenceClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for species classification with integer labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sequence = row[\"sequence\"]\n",
    "        label = torch.tensor(row[\"genus_encoded\"], dtype=torch.long)  # Long for classification\n",
    "\n",
    "        # Tokenize sequence\n",
    "        inputs = self.tokenizer(\n",
    "            sequence,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Remove batch dimension\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dae46bbc-4aa0-406a-b095-60a5dd778746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classification Model with LoRA\n",
    "class DNABERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines LoRA-adapted DNABERT with classification head.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, num_classes, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model  # LoRA-wrapped DNABERT\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        # Get embeddings from DNABERT\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Apply dropout and classification head\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2106c6a-8769-432f-965f-540e4c3ba8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Complete Model Architecture:\n",
      "------------------------------------------------------------\n",
      "  Trainable params: 3,615,454\n",
      "  Total params: 92,802,526\n",
      "  Trainable: 3.90%\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ“ DataLoaders created:\n",
      "  Train batches: 174\n",
      "  Val batches: 22\n",
      "  Test batches: 22\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Model\n",
    "classification_model = DNABERTClassifier(\n",
    "    base_model=ft_model,  # Your LoRA-adapted model\n",
    "    num_classes=num_classes,\n",
    "    hidden_size=768  # DNABERT-2-117M hidden size\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Complete Model Architecture:\")\n",
    "print(\"-\" * 60)\n",
    "print_trainable_parameters(classification_model)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = SequenceClassificationDataset(train_df, tokenizer)\n",
    "val_dataset = SequenceClassificationDataset(val_df, tokenizer)\n",
    "test_dataset = SequenceClassificationDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ecf7662-11e4-4ba0-83c5-5fa0692ec23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Training setup complete!\n",
      "  Optimizer: AdamW (lr=0.0002)\n",
      "  Loss: CrossEntropyLoss\n",
      "  Scheduler: CosineAnnealingLR\n"
     ]
    }
   ],
   "source": [
    "# Setup Training Components\n",
    "# Optimizer - only update trainable parameters\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, classification_model.parameters()), \n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Loss function for multi-class classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "print(\"\\nâœ“ Training setup complete!\")\n",
    "print(f\"  Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "print(f\"  Loss: CrossEntropyLoss\")\n",
    "print(f\"  Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4d5abf4-ecb5-42ff-94b4-1ad1d46ff994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    for inputs, labels in progress_bar:\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(**inputs)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    \"\"\"Evaluate model on validation/test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(**inputs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32956e2-fc31-4928-95a4-ea50e72a8454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Training!\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/174 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 58/174 [00:38<01:15,  1.54it/s, loss=7.3531, acc=4.54%]"
     ]
    }
   ],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸš€ Starting Training!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        classification_model, train_loader, optimizer, loss_fn, device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = evaluate(\n",
    "        classification_model, val_loader, loss_fn, device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nðŸ“Š Epoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc * 100:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc * 100:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(classification_model.state_dict(), 'best_model.pt')\n",
    "        print(f\"  âœ“ New best model saved! (Val Acc: {val_acc * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1e205-aaf6-4c8c-8daf-0db128ab29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Final Evaluation on Test Set\n",
    "print(\"\\nðŸ§ª Evaluating on test set...\")\n",
    "classification_model.load_state_dict(torch.load('best_model.pt'))\n",
    "test_loss, test_acc = evaluate(classification_model, test_loader, loss_fn, device)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Final Test Results:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 9: Plot Training History\n",
    "# =============================================================================\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot([acc * 100 for acc in history['train_acc']], label='Train Acc', marker='o')\n",
    "ax2.plot([acc * 100 for acc in history['val_acc']], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Training history plot saved as 'training_history.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
