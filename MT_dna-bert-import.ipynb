{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5c10a76-09f2-431a-b169-4e56e5664dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Data processing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Utilities\n",
    "import gc\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "# Set style for prettier plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31316e20-81b0-4ff5-96b2-5a4eb126aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "model = AutoModel.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e9973e8-123c-40e5-8542-012ed602fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       genus       species  \\\n",
       "0               Alitibacter   langaaensis   \n",
       "1               Alitibacter   langaaensis   \n",
       "2               Roseovarius     maritimus   \n",
       "3               Roseovarius        roseus   \n",
       "4           Planosporangium      spinosum   \n",
       "...                     ...           ...   \n",
       "27727     Thermoclostridium  stercorarium   \n",
       "27728           Clostridium      isatidis   \n",
       "27729         Couchioplanes     caeruleus   \n",
       "27730             Halomonas     koreensis   \n",
       "27731  Pseudoflavonifractor   phocaeensis   \n",
       "\n",
       "                                                sequence   identifier  \\\n",
       "0      ATTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCT...  NR_118751.1   \n",
       "1      ATTGAACGCTGGCGGCAGGCTTAACACATGCAAGTCGAACGGTAAC...  NR_042885.1   \n",
       "2      CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...  NR_200035.1   \n",
       "3      CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...  NR_200034.1   \n",
       "4      TTGTTGGAGAGTTTGATCCTGGCTCAGGACGAACGCTGGCGGCGTG...  NR_200033.1   \n",
       "...                                                  ...          ...   \n",
       "27727  TGATCCTGGCTCAGGACGAACGCTGGCGGCGTGCCTAACACATGCA...  NR_025100.1   \n",
       "27728  GGCGTGCNTAACACATGCAAGTCGAGCGAGGTGATTTCNTTCGGGA...  NR_026347.1   \n",
       "27729  CGCTGGCGGCGTGCTTAACACATGCAAGTCGAGCGGAAAGGCCCCT...  NR_026295.1   \n",
       "27730  ACGATGGGAGCTTGCTCCCAGGCGTCGAGCGGCGGACGGGTGAGTA...  NR_025773.1   \n",
       "27731  AGAGTTTGATCCTGGCTCAGGATGAACGCTGGCGGCGTRCTTAACA...  NR_147370.1   \n",
       "\n",
       "             is_complete  \n",
       "0       partial sequence  \n",
       "1       partial sequence  \n",
       "2      complete sequence  \n",
       "3      complete sequence  \n",
       "4      complete sequence  \n",
       "...                  ...  \n",
       "27727   partial sequence  \n",
       "27728   partial sequence  \n",
       "27729   partial sequence  \n",
       "27730   partial sequence  \n",
       "27731   partial sequence  \n",
       "\n",
       "[27732 rows x 5 columns]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the new sequence data and run head to see the overall data structure\n",
    "data_in = pd.read_csv(\"data/sequence-wide.tsv\", sep='\\t') \n",
    "data_in.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb1efdce-56aa-428f-a633-b6d28c915dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 1:\n",
      "0                 Alitibacter\n",
      "1                 Alitibacter\n",
      "2                 Roseovarius\n",
      "3                 Roseovarius\n",
      "4             Planosporangium\n",
      "                 ...         \n",
      "27727       Thermoclostridium\n",
      "27728             Clostridium\n",
      "27729           Couchioplanes\n",
      "27730               Halomonas\n",
      "27731    Pseudoflavonifractor\n",
      "Name: genus, Length: 27732, dtype: object\n",
      "Column 2:\n",
      "0         langaaensis\n",
      "1         langaaensis\n",
      "2           maritimus\n",
      "3              roseus\n",
      "4            spinosum\n",
      "             ...     \n",
      "27727    stercorarium\n",
      "27728        isatidis\n",
      "27729       caeruleus\n",
      "27730       koreensis\n",
      "27731     phocaeensis\n",
      "Name: species, Length: 27732, dtype: object\n",
      "Column 3:\n",
      "0        ATTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCT...\n",
      "1        ATTGAACGCTGGCGGCAGGCTTAACACATGCAAGTCGAACGGTAAC...\n",
      "2        CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...\n",
      "3        CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...\n",
      "4        TTGTTGGAGAGTTTGATCCTGGCTCAGGACGAACGCTGGCGGCGTG...\n",
      "                               ...                        \n",
      "27727    TGATCCTGGCTCAGGACGAACGCTGGCGGCGTGCCTAACACATGCA...\n",
      "27728    GGCGTGCNTAACACATGCAAGTCGAGCGAGGTGATTTCNTTCGGGA...\n",
      "27729    CGCTGGCGGCGTGCTTAACACATGCAAGTCGAGCGGAAAGGCCCCT...\n",
      "27730    ACGATGGGAGCTTGCTCCCAGGCGTCGAGCGGCGGACGGGTGAGTA...\n",
      "27731    AGAGTTTGATCCTGGCTCAGGATGAACGCTGGCGGCGTRCTTAACA...\n",
      "Name: sequence, Length: 27732, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the columns\n",
    "print(\"Column 1:\")\n",
    "print(data_in.iloc[:, 0])\n",
    "print(\"Column 2:\")\n",
    "print(data_in.iloc[:, 1])\n",
    "print(\"Column 3:\")\n",
    "print(data_in.iloc[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90b6a90c-5dc6-4402-b207-c4ec3f1647b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ATTGAAGAGTTTGATCATGGCTCAGATTGAACGCTGGCGGCAGGCT...\n",
       "1        ATTGAACGCTGGCGGCAGGCTTAACACATGCAAGTCGAACGGTAAC...\n",
       "2        CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...\n",
       "3        CAACTTGAGAGTTTGATCCTGGCTCAGAACGAACGCTGGCGGCAGG...\n",
       "4        TTGTTGGAGAGTTTGATCCTGGCTCAGGACGAACGCTGGCGGCGTG...\n",
       "                               ...                        \n",
       "27727    TGATCCTGGCTCAGGACGAACGCTGGCGGCGTGCCTAACACATGCA...\n",
       "27728    GGCGTGCNTAACACATGCAAGTCGAGCGAGGTGATTTCNTTCGGGA...\n",
       "27729    CGCTGGCGGCGTGCTTAACACATGCAAGTCGAGCGGAAAGGCCCCT...\n",
       "27730    ACGATGGGAGCTTGCTCCCAGGCGTCGAGCGGCGGACGGGTGAGTA...\n",
       "27731    AGAGTTTGATCCTGGCTCAGGATGAACGCTGGCGGCGTRCTTAACA...\n",
       "Name: sequence, Length: 27732, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71b93154-30bd-411a-a82b-e6dd4026314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2061, 25, 222, 23, 224, 143, 3411, 403, 247, 53, 150, 527, 2759, 2834, 2734, 724, 873, 81, 118, 2470, 30, 708, 72, 61, 679, 29, 200, 88, 2894, 71, 117, 639, 72, 1478, 315, 137, 2787, 825, 1826, 966, 189, 1235, 45, 229, 4079, 314, 1340, 835, 427, 138, 316, 99, 120, 2139, 76, 36, 987, 75, 315, 8, 0, 41, 199, 0, 0, 5, 778, 460, 632, 59, 100, 72, 26, 0, 1212, 1527, 71, 148, 281, 0, 9, 3558, 238, 92, 635, 59, 111, 556, 2787, 135, 52, 259, 64, 72, 31, 120, 469, 2816, 50, 2638, 166, 29, 135, 1262, 31, 141, 17, 495, 1170, 317, 32, 443, 79, 78, 30, 619, 36, 247, 137, 1517, 2810, 19, 153, 1826, 20, 277, 1080, 332, 159, 15, 583, 458, 61, 783, 18, 486, 17, 540, 29, 200, 14, 183, 22, 236, 168, 37, 282, 3453, 71, 7, 0, 3386, 34, 123, 315, 103, 265, 194, 50, 42, 534, 171, 259, 166, 112, 2394, 200, 106, 59, 118, 1952, 409, 577, 117, 124, 1832, 0, 113, 205, 35, 553, 403, 38, 499, 16, 605, 788, 212, 8, 0, 0, 9, 3382, 169, 194, 233, 368, 38, 2785, 1149, 282, 1435, 66, 101, 39, 386, 8, 0, 9, 10, 846, 599, 159, 27, 1317, 208, 3928, 109, 499, 0, 9, 118, 317, 292, 114, 149, 100, 462, 2093, 2641, 14, 163, 2117, 72, 212, 126, 823, 472, 65, 62, 1440, 239, 163, 327, 48, 342, 0, 5, 90, 547, 93, 1818, 1128, 130, 48, 89, 266, 8, 0, 9, 1731, 386, 33, 251, 1957, 8, 0, 9, 438, 57, 885, 245, 1191, 67, 224, 3953, 93, 522, 3315, 123, 37, 238, 30, 104, 2276, 247, 166, 74, 393, 61, 841, 116, 3345, 278, 35, 90, 216, 33, 220, 2485, 3770, 52, 41, 57, 2957, 68, 16, 124, 455, 48, 247, 1092, 134, 716, 184, 2146, 1002, 397, 2596, 824, 101, 35, 637, 3642, 540, 347, 165, 764, 268, 72, 252, 115, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "# Convert the new sequences to input tokens, view the first element\n",
    "inputs = tokenizer(data_in.sequence.to_list())[\"input_ids\"]\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb5d480c-bc7c-4a53-af57-4f096a1deedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Random seeds set for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Set all random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3148d170-34b8-472b-b413-96254b368c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Applying LoRA adapters to model...\n",
      "\n",
      "ðŸ“Š Model Parameters Comparison:\n",
      "------------------------------------------------------------\n",
      "Before LoRA (original model):\n",
      "  Trainable params: 294,912\n",
      "  Total params: 89,481,984\n",
      "  Trainable: 0.33%\n",
      "\n",
      "After LoRA (adapted model):\n",
      "  Trainable params: 294,912\n",
      "  Total params: 89,481,984\n",
      "  Trainable: 0.33%\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ“ LoRA adapters applied successfully!\n"
     ]
    }
   ],
   "source": [
    "# Helper function to count trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Count and display trainable vs total parameters.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    trainable_pct = 100 * trainable_params / all_param\n",
    "    print(f\"  Trainable params: {trainable_params:,}\")\n",
    "    print(f\"  Total params: {all_param:,}\")\n",
    "    print(f\"  Trainable: {trainable_pct:.2f}%\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                              # Rank (lower = fewer params)\n",
    "    lora_alpha=32,                    # Scaling factor\n",
    "    target_modules=[\"query\", \"value\"], # Adapt attention layers\n",
    "    lora_dropout=0.1,                 # Regularization\n",
    "    bias=\"none\",                      # Don't adapt bias terms\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(\"ðŸ”„ Applying LoRA adapters to model...\\n\")\n",
    "ft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"ðŸ“Š Model Parameters Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Before LoRA (original model):\")\n",
    "print_trainable_parameters(model)\n",
    "print(\"\\nAfter LoRA (adapted model):\")\n",
    "print_trainable_parameters(ft_model)\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nâœ“ LoRA adapters applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58a1f380-a9fc-4bdf-9c71-e61d4272d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train-val-test splits\n",
    "# Configuration\n",
    "prop_train = 0.8\n",
    "prop_val = 0.1\n",
    "prop_test = 0.1\n",
    "SEED = 42\n",
    "\n",
    "# Create random splits\n",
    "rng = np.random.default_rng(SEED)\n",
    "random_idxs = rng.permutation(len(data_in))\n",
    "\n",
    "# Calculate split sizes\n",
    "n_total = len(data_in)\n",
    "n_train = int(prop_train * n_total)\n",
    "n_val = int(prop_val * n_total)\n",
    "\n",
    "train_df = data_in.iloc[random_idxs[:n_train]]\n",
    "val_df = data_in.iloc[random_idxs[n_train:n_train + n_val]]\n",
    "test_df = data_in.iloc[random_idxs[n_train + n_val:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f2e83bc-15ed-44cf-9c45-c9bd606a348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SequenceDataset class defined\n"
     ]
    }
   ],
   "source": [
    "# Custom PyTorch Dataset for protein sequences\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that tokenizes protein sequences on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sequence = row[\"sequence\"]\n",
    "        label = torch.tensor(row[\"species\"], dtype=torch.float32)\n",
    "\n",
    "        # Tokenize sequence\n",
    "        inputs = self.tokenizer(\n",
    "            sequence,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        # Remove batch dimension\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs, label\n",
    "\n",
    "print(\"âœ“ SequenceDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c482cd2-2279-4909-90a1-4923849ab3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Datasets created:\n",
      "  Train: 22,185 samples\n",
      "  Val: 2,773 samples\n",
      "  Test: 2,774 samples\n"
     ]
    }
   ],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = SequenceDataset(train_df, tokenizer)\n",
    "val_dataset = SequenceDataset(val_df, tokenizer)\n",
    "test_dataset = SequenceDataset(test_df, tokenizer)\n",
    "\n",
    "print(\"âœ“ Datasets created:\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples\")\n",
    "print(f\"  Val: {len(val_dataset):,} samples\")\n",
    "print(f\"  Test: {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e15da4-24e8-4722-87b6-173918eb9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Create custom model with regression head\n",
    "class ClassificationModel(nn.Module):\n",
    "    \"\"\"Combines LoRA-adapted DNA BERT model with classification head.\"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = model  # LoRA-wrapped model\n",
    "        self.regressor = nn.Linear(base_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.base_model(**inputs)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.regressor(pooled_output).squeeze(-1)\n",
    "\n",
    "# Instantiate the complete model\n",
    "regression_model = RegressionModel(ft_model).to(device)\n",
    "\n",
    "print(\"ðŸŽ¯ Complete Model Architecture:\")\n",
    "print(\"-\" * 60)\n",
    "print_trainable_parameters(regression_model)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create data loaders\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "train_dataset = SequenceDataset(train_df, tokenizer)\n",
    "val_dataset = SequenceDataset(val_df, tokenizer)\n",
    "test_dataset = SequenceDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Setup optimizer and loss\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, regression_model.parameters()), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"\\nâœ“ Model and data loaders ready for training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
